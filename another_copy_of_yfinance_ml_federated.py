# -*- coding: utf-8 -*-
"""Another copy of YFinance ML Federated

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fi8eZ4R4RV6-DCCJd0F_vqilwcYbZb5k
"""

import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
!pip install scikeras
from scikeras.wrappers import KerasRegressor
from datetime import datetime, timedelta

"""
Takes a stock ticker and creates and runs an LSTM regressive neural network model.
It uses the last 20 years of stock data from yfinance api and splits 15 into train and 5 into test.
This function returns the mean absolute percent error of the model.
"""
def runModel(ticker):

    data = yf.download(tickers=ticker, start="2004-07-20", end="2024-07-20")

    #Select the 'Close' prices as the target variable
    data = data.asfreq("D")
    data = data[['Adj Close']]

    #Preprocessing (filling missing values, scaling)
    data = data.fillna(method='ffill')

    # Split the data into training and test sets
    split_date = '2019-07-18'
    split_date_2 = '2019-07-19'
    train_data = data[:split_date]
    test_data = data[split_date_2:]

    #Scaling data to standardize to 0-1
    scaler = MinMaxScaler()
    train_data_scaled = scaler.fit_transform(train_data)
    test_data_scaled = scaler.transform(test_data)

    #Create sequences for training
    sequence_length = 10

    X_train, y_train = [], []
    X_test, y_test = [], []

    for i in range(len(train_data_scaled)-sequence_length):
        X_train.append(train_data_scaled[i:i+sequence_length])
        y_train.append(train_data_scaled[i+sequence_length])

    for i in range(len(test_data_scaled)-sequence_length):
        X_test.append(test_data_scaled[i:i+sequence_length])
        y_test.append(test_data_scaled[i+sequence_length])

    X_train, y_train = np.array(X_train), np.array(y_train)
    X_test, y_test = np.array(X_test), np.array(y_test)

    #Define a function to create the model
    def create_model(optimizer="adam", units=64):
        model1 = tf.keras.Sequential([
            tf.keras.layers.LSTM(units, input_shape=(sequence_length, 1), return_sequences=True),
            tf.keras.layers.LSTM(units),
            tf.keras.layers.Dense(1)
        ])
        model1.compile(optimizer=optimizer, loss="mean_squared_error")
        return model1

    model2 = create_model(optimizer="adam", units=64)

    #perform gridsearch onmodel
    model_LSTM = KerasRegressor(model=create_model, verbose=0)

    #Long short-term memory (LSTM) model creation
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(64, input_shape=(sequence_length, 1), return_sequences=True),
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Dense(1)
    ])

    #Compile the model
    model.compile(optimizer="adam", loss="mean_squared_error")

    #Fit the model
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

    #Predict using the model
    predicted_values = model.predict(X_test)

    #Inverse Scaling
    predicted_values = scaler.inverse_transform(predicted_values)
    y_test_inv = scaler.inverse_transform(y_test)

    # percent_increasePRED = (predicted_values[-1] - predicted_values[0]) / predicted_values[0] * 100

    # return percent_increasePRED

    #Calculate Mean Absolute Percentage Error
    def calculate_mape(y_true, y_pred):
        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

    mape = calculate_mape(y_test_inv, predicted_values)
    return mape

"""
Creates a comparison line chart of the actual and predicted values of the model.
"""

def create_graph(ticker):
    data = yf.download(tickers=ticker, start="2004-07-20", end="2024-07-20")

    #Select the 'Close' prices as the target variable
    data = data.asfreq("D")
    data = data[['Adj Close']]

    #Preprocessing (filling missing values, scaling)
    data = data.fillna(method='ffill')

    # Split the data into training and test sets
    split_date = '2019-07-18'
    split_date_2 = '2019-07-19'
    train_data = data[:split_date]
    test_data = data[split_date_2:]

    #Scaling data to standardize to 0-1
    scaler = MinMaxScaler()
    train_data_scaled = scaler.fit_transform(train_data)
    test_data_scaled = scaler.transform(test_data)

    #Create sequences for training
    sequence_length = 10

    X_train, y_train = [], []
    X_test, y_test = [], []

    for i in range(len(train_data_scaled)-sequence_length):
        X_train.append(train_data_scaled[i:i+sequence_length])
        y_train.append(train_data_scaled[i+sequence_length])

    for i in range(len(test_data_scaled)-sequence_length):
        X_test.append(test_data_scaled[i:i+sequence_length])
        y_test.append(test_data_scaled[i+sequence_length])

    X_train, y_train = np.array(X_train), np.array(y_train)
    X_test, y_test = np.array(X_test), np.array(y_test)

    #Define a function to create the model
    def create_model(optimizer="adam", units=64):
        model1 = tf.keras.Sequential([
            tf.keras.layers.LSTM(units, input_shape=(sequence_length, 1), return_sequences=True),
            tf.keras.layers.LSTM(units),
            tf.keras.layers.Dense(1)
        ])
        model1.compile(optimizer=optimizer, loss="mean_squared_error")
        return model1

    model2 = create_model(optimizer="adam", units=64)

    #perform gridsearch onmodel
    model_LSTM = KerasRegressor(model=create_model, verbose=0)

    #Long short-term memory (LSTM) model creation
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(64, input_shape=(sequence_length, 1), return_sequences=True),
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Dense(1)
    ])

    #Compile the model
    model.compile(optimizer="adam", loss="mean_squared_error")

    #Fit the model
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

    #Predict using the model
    predicted_values = model.predict(X_test)

    #Inverse Scaling
    predicted_values = scaler.inverse_transform(predicted_values)
    y_test_inv = scaler.inverse_transform(y_test)


    # Assuming 'test_data' DataFrame has a 'Date' column
    dates = test_data.index[sequence_length:]  # Adjust for sequence length

    plt.figure(figsize=(12, 6))
    plt.plot(dates, y_test_inv, label='Actual')
    plt.plot(dates, predicted_values, label='Predicted')
    plt.xlabel('Date')
    plt.ylabel('Stock Price')
    plt.title('Stock Price Prediction')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

def get_market_cap_now(ticker_symbol):
    stock = yf.Ticker(ticker_symbol)
    market_cap = stock.info.get('marketCap')
    return market_cap

def get_market_cap_5_years_ago(ticker_symbol):
    #Get the date 5 years ago
    date_5_years_ago = datetime.now() - timedelta(days=5*365)

    #Get the stock data
    stock = yf.Ticker(ticker_symbol)

    #Get historical data for the date 5 years ago
    historical_data = stock.history(start=date_5_years_ago, end=date_5_years_ago + timedelta(days=1))

    if historical_data.empty:
        print(f"No historical data available for {ticker_symbol}")
        return None

    closing_price = historical_data['Close'][0]
    shares_outstanding = stock.info.get('sharesOutstanding', None)

    if shares_outstanding is None:
        print(f"Number of shares outstanding not available for {ticker_symbol}")
        return None

    market_cap_5_years_ago = closing_price * shares_outstanding

    return market_cap_5_years_ago

"""
Returns tickers of S&P 500 stocks in an np array
"""
def getSP500():
    table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
    df = table[0]
    tickers = df['Symbol'].to_numpy()
    return tickers

"""
Returns the largest 50 stocks 5 years ago as a dataframe
"""

def getLargetsMarketCaps5YearsAgo():
    tickers = getSP500()

#adding the market cap for each ticker to a dict
    market_caps = {}
    for ticker in tickers:
        market_cap = get_market_cap_5_years_ago(ticker)
        if market_cap is not None:
            market_caps[ticker] = market_cap

    # Sort the market caps in descending order and get the top 50
    top_50_market_caps = dict(sorted(market_caps.items(), key=lambda item: item[1], reverse=True)[:50])

    # Create a DataFrame from the top 50 market caps
    top_50_df = pd.DataFrame.from_dict(top_50_market_caps, orient='index', columns=['Market Cap 5 Years Ago'])
    top_50_df.index.name = 'Ticker'
    top_50_df = top_50_df.sort_values(by='Market Cap 5 Years Ago', ascending=False)

    return top_50_df

# Get the largest 50 stocks 5 years ago
top_50_df = getLargetsMarketCaps5YearsAgo()

# Initialize arrays to store results
tickers_array = top_50_df.index.to_numpy()
market_caps_array = np.empty(len(tickers_array))
model_outputs_array = np.empty(len(tickers_array))

# Iterate over tickers and calculate market caps and model outputs
for i, ticker in enumerate(tickers_array):
  market_caps_array[i] = get_market_cap_now(ticker)
  model_outputs_array[i] = runModel(ticker)

# Create a DataFrame from the arrays
result_df = pd.DataFrame({
    'Ticker': tickers_array,
    'Market Cap': market_caps_array,
    'Model Output': model_outputs_array
})

print(result_df)

# Calculate the median MAPE and find the corresponding ticker
median_mape = np.median(result_df['Model Output'])
closest_mape_idx = np.argmin(np.abs(result_df['Model Output'] - median_mape))
median_mape_ticker = result_df.iloc[closest_mape_idx]['Ticker']
print("Ticker with the closest MAPE to the median:", median_mape_ticker)

result_df_sorted = result_df.sort_values('Model Output')
result_df_sorted.head(10)

result_df_sorted.tail(10)

create_graph('GOOG')

create_graph('PM')

create_graph('CRM')

# @title Model Output

from matplotlib import pyplot as plt
result_df_sorted['Model Output'].plot(kind='hist', bins=20, title='Model Output')
plt.gca().spines[['top', 'right',]].set_visible(False)

data = yf.download(tickers='NVDA', start="2004-07-21", end="2024-07-21")

#Select the 'Close' prices as the target variable
data = data.asfreq("D")
data = data[['Adj Close']]

#Preprocessing (filling missing values, scaling)
data = data.fillna(method='ffill')

split_date = '2019-07-18'
split_date_2 = '2019-07-19'
train_data = data[:split_date]
test_data = data[split_date_2:]

#Scaling data to standardize to 0-1
scaler = MinMaxScaler()
train_data_scaled = scaler.fit_transform(train_data)
test_data_scaled = scaler.transform(test_data)


#Create sequences for training
sequence_length = 10

X_train, y_train = [], []
X_test, y_test = [], []

for i in range(len(train_data_scaled)-sequence_length):
    X_train.append(train_data_scaled[i:i+sequence_length])
    y_train.append(train_data_scaled[i+sequence_length])

for i in range(len(test_data_scaled)-sequence_length):
    X_test.append(test_data_scaled[i:i+sequence_length])
    y_test.append(test_data_scaled[i+sequence_length])

X_train, y_train = np.array(X_train), np.array(y_train)
X_test, y_test = np.array(X_test), np.array(y_test)

print('\n')
print(train_data_scaled)

print(y_test[0])

